{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itC6u5MwXTIu"
   },
   "source": [
    "# Steam Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tB7Ot-2XTI2"
   },
   "source": [
    "## Project Goals\n",
    "\n",
    "<!-- PELICAN_BEGIN_SUMMARY -->\n",
    "\n",
    "The motivation is gather, process and analyze Steam Store data to get insights about trends in the videogame market. As it is an online marketplace with public available data, it offers us more possibilities than analyzing console games data, where we would have to rely on an existing dataset.\n",
    "\n",
    "We want to focus on two main aspects, first a general market analysis to know which genres are the most popular, pricing strategies and so on, which could be interesting for a new developer trying to make a new game or deciding a price policy. This has been studied already by other enthusiasts in internet, and also by Marketing companies helping publishers.\n",
    "\n",
    "But to offer a different analysis, we want to also focus on the developers and publishers, to see which ones are the most successfull, how they have improved / worsen between the years, which titles have cemented their success and so on. In the light of recent years we have seen many acquisitions by large publishers such as Tencent, Microsoft and Sony, so this is very interesting concept.\n",
    "\n",
    "This will be a complete data project, with a data acquisition section (by using some APIs and web scrapping), then data cleaning and joining data from different sources, an exploratory data analysis, and finally some key conclusions.\n",
    "\n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "This is the section where I struggled initially. There were several datasets available at [kaggle](https://www.kaggle.com/datasets), reddit and similar websites, but most were outdated or did not contain all the information I wanted to explore. Also I wanted to extract it directly from an API or use web scrapping, if possible, to learn a bit more (I already had experience with Twitter which has an excellent API).\n",
    "\n",
    "[SteamSpy](https://steamspy.com/about) is a webpage which offers data about Steam games. In the past it was even able to deliver a good guess of sales, but that has become harder throughout the years. Check [VG Insights](https://vginsights.com/insights/article/how-to-estimate-steam-video-game-sales) for more information. It is also a good webpage if you want to explore market data on your own.\n",
    "The most important thing for Steamspy is that it has its own API [here](https://steamspy.com/api.php). It can provide us easily with an already filtered list of games (not other apps or DLCs), and also some metrics not available at Steam directly such as an estimate of sales and the positive or negative reviews (Steam only gives us total number reviews).\n",
    "\n",
    "Regarding Steam directly, the API is available at https://partner.steamgames.com/ , however you need a developer key and some (most of the functions) are tied to your key as they are intended to be used to manage your own products at the Steam store. Thanks to [Nik Davis](http://nik-davis.github.io) I discovered there were also a few API functions via the WEB API which can be used without a key at all. See here for more details: [StorefrontAPI](https://wiki.teamfortress.com/wiki/User:RJackson/StorefrontAPI).\n",
    "\n",
    "Getting the information from Steam will be a bit more difficult, but it will give us additional metrics, such as release date, genre...\n",
    "\n",
    "We will retrieve first the list of appids and the information available at Steam Spy, then get for each appid the information from Steam and combine them in an unique dataframe. There will be no loss of information as app ids are unique. Afterwards, we will perform cleaning and finally start analyzing our dataset.\n",
    "\n",
    "## Process:\n",
    "\n",
    "- Create an app list and gather available data from SteamSpy API using 'all' request\n",
    "- Retrieve individual app data from Steam API, by iterating through app list\n",
    "- Export app list, Steam data and SteamSpy data to csv files\n",
    "\n",
    "## API references:\n",
    "\n",
    "- https://partner.steamgames.com/doc/webapi\n",
    "- https://wiki.teamfortress.com/wiki/User:RJackson/StorefrontAPI\n",
    "- https://steamapi.xpaw.me/#\n",
    "- https://steamspy.com/api.php\n",
    "\n",
    "\n",
    "## Credits\n",
    "\n",
    "The most important source I found while looking how to connect to the API was Nik Davis, check his blog for a different analysis on steam data (from 2019) http://nik-davis.github.io\n",
    "Download functions for the APIs are based on his notebook for \"Steam Data Download\". I had to make some changes and simplify a bit.\n",
    "\n",
    "Steamspy seems to have changed its API, so I had to change the download method to instead download all the data by page (set of 1000 ids). The functions defined for Steam API itself still work as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cB_ucVTyXTI3"
   },
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import csv\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "# third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# customisations - ensure tables show all columns\n",
    "pd.set_option(\"max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGVIbrCJXTI4"
   },
   "source": [
    "The next function uses requests library to get JSON response from web APIs. It is based on Nik Davis previous work, and it is quite standard as (thankfully) web APIs use a standard format, and requests makes it really easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hggyD3aNXTI5"
   },
   "outputs": [],
   "source": [
    "def get_request(url,parameters=None, steamspy=False):\n",
    "    \"\"\"Return json-formatted response of a get request using optional parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "    parameters : {'parameter': 'value'}\n",
    "        parameters to pass as part of get request\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    json_data\n",
    "        json-formatted response (dict-like)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url=url, params=parameters)\n",
    "    except SSLError as s:\n",
    "        print('SSL Error:', s)\n",
    "        \n",
    "        for i in range(5, 0, -1):\n",
    "            print('\\rWaiting... ({})'.format(i), end='')\n",
    "            time.sleep(1)\n",
    "        print('\\rRetrying.' + ' '*10)\n",
    "        \n",
    "        # recursively try again\n",
    "        return get_request(url, parameters, steamspy)\n",
    "    \n",
    "    if response:\n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            False\n",
    "    else:\n",
    "        # We do not know how many pages steamspy has... and it seems to work well, so we will use no response to stop.\n",
    "        if steamspy:\n",
    "            return \"stop\"\n",
    "        else :\n",
    "            # response is none usually means too many requests. Wait and try again \n",
    "            print('No response, waiting 10 seconds...')\n",
    "            time.sleep(10)\n",
    "            print('Retrying.')\n",
    "            return get_request(url, parameters, steamspy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfR35UFwXTI5"
   },
   "source": [
    "## List of IDs\n",
    "\n",
    "APPs on steam have an unique ID. The requests to Steam API (which has more information than Steam Spy) have to be made for a specific ID. This means we have to get first a list of ids.\n",
    "\n",
    "We can do this in several ways, but this is what I decided to follow:\n",
    "\n",
    "* Using Steam Spy API (see https://steamspy.com/api.php) to get the list of IDs and also the metadata from Steam Spy (at the same time. Unfortunately, using this method got me lots of duplicates and headaches.\n",
    "\n",
    "\n",
    "* Alternatively, we could use Steam API to get a list of apps, then filter them (see https://api.steampowered.com/ISteamApps/GetAppList/v2/? or https://steamapi.xpaw.me/#IStoreService/GetAppInfo)\n",
    "\n",
    "We will do the last alternative, and then get the information from Steam Spy for the available IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not know how many appids were registered in steamspy at 26/1/2022 (steamspy ranks them by user average, it seems), and it seems there are 51k appids.\n",
    "\n",
    "The code above stops if it fails using the steamspy flag, because steamspy API is quite generous and has let us download all the data with just 1s pause between requests (although in their API it says we should wait 60s for a full page request).\n",
    "\n",
    "Instead of telling us that a new page does not exist (by returning a null, in example) it gives us a server error, so this is a way to do it. Steam just returns null in this scenario.\n",
    "\n",
    "I have left the data from steamspy below, in any case."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_steam_spy = pd.read_csv(\"../data/download/steamspy_appid.csv\")\n",
    "# generate sorted app_list from steamspy data\n",
    "app_list = df_steam_spy[['appid', 'name']].sort_values('appid').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_steam_spy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_steam_spy[\"appid\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking the duplicated by appid, we can already see that there is some issue with this approach. It would be better to get the full steam list from Steam Store, and come back to see if we can get the data individually from Steam Spy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx8MClSGXTI6"
   },
   "source": [
    "## Define Download Logic\n",
    "\n",
    "This is strongly based on Nik Davis previous work, to get the info about the app IDs from Steam. Initially I prefered to focus on the analysis rather than in the acquisition phase, but I watn to modify these functions to change them from an index based approach for the update, to instead check already existing ids in the database and download only the delta.\n",
    "\n",
    "Later, we could maybe also add a check to see if the app has been updated or not and even redownload the info from not just new IDs, but IDs that have been updated.\n",
    "\n",
    "I will keep the original comments from Nik Davis `in quotes` to let the reader understand the process.\n",
    "\n",
    "`Now we have the app_list dataframe, we can iterate over the app IDs and request individual app data from the servers. Here we set out our logic to retrieve and process this information, then finally store the data as a csv file.`\n",
    "\n",
    "`Because it takes a long time to retrieve the data, it would be dangerous to attempt it all in one go as any errors or connection time-outs could cause the loss of all our data. For this reason we define a function to download and process the requests in batches, appending each batch to an external file and keeping track of the highest index written in a separate file.`\n",
    "\n",
    "`This not only provides security, allowing us to easily restart the process if an error is encountered, but also means we can complete the download across multiple sessions.`\n",
    "\n",
    "`Again, we provide verbose output for rows exported, batches complete, time taken and estimated time remaining.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1eLPdKoGXTI6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_app_data(app_list, start, stop, parser, pause):\n",
    "    \"\"\"Return list of app data generated from parser.\n",
    "    \n",
    "    parser : function to handle request\n",
    "    \"\"\"\n",
    "    app_data = []\n",
    "    \n",
    "    # iterate through each row of app_list, confined by start and stop\n",
    "    for index, appid in app_list[start:stop].iteritems():\n",
    "        print('Current index: {}'.format(index), end='\\r')\n",
    "\n",
    "        # retrive app data for a row, handled by supplied parser, and append to list\n",
    "        try:\n",
    "            data = parser(appid)\n",
    "            app_data.append(data)\n",
    "        except:\n",
    "            print(\"Error with \"+str(appid))\n",
    "        time.sleep(pause) # prevent overloading api with requests\n",
    "    \n",
    "    return app_data\n",
    "\n",
    "\n",
    "def process_batches(parser, app_list, download_path, data_filename, index_filename,\n",
    "                    columns, begin=0, end=-1, batchsize=100, pause=1):\n",
    "    \"\"\"Process app data in batches, writing directly to file.\n",
    "    \n",
    "    parser : custom function to format request\n",
    "    app_list : dataframe of appid and name\n",
    "    download_path : path to store data\n",
    "    data_filename : filename to save app data\n",
    "    index_filename : filename to store highest index written\n",
    "    columns : column names for file\n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    begin : starting index (get from index_filename, default 0)\n",
    "    end : index to finish (defaults to end of app_list)\n",
    "    batchsize : number of apps to write in each batch (default 100)\n",
    "    pause : time to wait after each api request (defualt 1)\n",
    "    \n",
    "    returns: none\n",
    "    \"\"\"\n",
    "    print('Starting at index {}:\\n'.format(begin))\n",
    "    \n",
    "    # by default, process all apps in app_list\n",
    "    if end == -1:\n",
    "        end = len(app_list) + 1\n",
    "    \n",
    "    # generate array of batch begin and end points\n",
    "    batches = np.arange(begin, end, batchsize)\n",
    "    batches = np.append(batches, end)\n",
    "    \n",
    "    apps_written = 0\n",
    "    batch_times = []\n",
    "    \n",
    "    for i in range(len(batches) - 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        start = batches[i]\n",
    "        stop = batches[i+1]\n",
    "        \n",
    "        app_data = get_app_data(app_list, start, stop, parser, pause)\n",
    "        \n",
    "        rel_path = os.path.join(download_path, data_filename)\n",
    "        \n",
    "        # writing app data to file\n",
    "        with open(rel_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns, extrasaction='ignore')\n",
    "            \n",
    "            for j in range(3,0,-1):\n",
    "                print(\"\\rAbout to write data, don't stop script! ({})\".format(j), end='')\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            writer.writerows(app_data)\n",
    "            print('\\rExported lines {}-{} to {}.'.format(start, stop-1, data_filename), end=' ')\n",
    "            \n",
    "        apps_written += len(app_data)\n",
    "        \n",
    "        idx_path = os.path.join(download_path, index_filename)\n",
    "        \n",
    "        # writing last index to file\n",
    "        with open(idx_path, 'w') as f:\n",
    "            index = stop\n",
    "            print(index, file=f)\n",
    "            \n",
    "        # logging time taken\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        batch_times.append(time_taken)\n",
    "        mean_time = statistics.mean(batch_times)\n",
    "        \n",
    "        est_remaining = (len(batches) - i - 2) * mean_time\n",
    "        \n",
    "        remaining_td = dt.timedelta(seconds=round(est_remaining))\n",
    "        time_td = dt.timedelta(seconds=round(time_taken))\n",
    "        mean_td = dt.timedelta(seconds=round(mean_time))\n",
    "        \n",
    "        print('Batch {} time: {} (avg: {}, remaining: {})'.format(i, time_td, mean_td, remaining_td))\n",
    "            \n",
    "    print('\\nProcessing batches complete. {} apps written'.format(apps_written))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to use this function and still only get the newer apps, would be to instead of passing it the fully app_list, preprocess it so it only contains the \"app_delta\". Also we would need it to keep the final dataframe in a different file, to perform after an append to it (in case we are adding only new app_ids), or if we add some kind of updating process, a join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP2UaFpyXTI7"
   },
   "source": [
    "`Next we define some functions to handle and prepare the external files.`\n",
    "\n",
    "`We use reset_index for testing and demonstration, allowing us to easily reset the index in the stored file to 0, effectively restarting the entire download process.`\n",
    "\n",
    "`We define get_index to retrieve the index from file, maintaining persistence across sessions. Every time a batch of information (app data) is written to file, we write the highest index within app_data that was retrieved. As stated, this is partially for security, ensuring that if there is an error during the download we can read the index from file and continue from the end of the last successful batch. Keeping track of the index also allows us to pause the download, continuing at a later time.`\n",
    "\n",
    "`Finally, the prepare_data_file function readies the csv for storing the data. If the index we retrieved is 0, it means we are either starting for the first time or starting over. In either case, we want a blank csv file with only the header row to begin writing to, se we wipe the file (by opening in write mode) and write the header. Conversely, if the index is anything other than 0, it means we already have downloaded information, and can leave the csv file alone.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "79z_mTOoXTI7"
   },
   "outputs": [],
   "source": [
    "def reset_index(download_path, index_filename):\n",
    "    \"\"\"Reset index in file to 0.\"\"\"\n",
    "    rel_path = os.path.join(download_path, index_filename)\n",
    "    \n",
    "    f= open(rel_path, 'w')\n",
    "    f.write(\"0\")\n",
    "        \n",
    "\n",
    "def get_index(download_path, index_filename):\n",
    "    \"\"\"Retrieve index from file, returning 0 if file not found.\"\"\"\n",
    "    try:\n",
    "        rel_path = os.path.join(download_path, index_filename)\n",
    "\n",
    "        with open(rel_path, 'r') as f:\n",
    "            index = int(f.readline())\n",
    "            #This just reads the initial line\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        index = 0\n",
    "        \n",
    "    return index\n",
    "\n",
    "\n",
    "def prepare_data_file(download_path, filename, index, columns):\n",
    "    \"\"\"Create file and write headers if index is 0.\"\"\"\n",
    "    if index == 0:\n",
    "        rel_path = os.path.join(download_path, filename)\n",
    "\n",
    "        with open(rel_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns)\n",
    "            writer.writeheader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnD4UGAnXTI7"
   },
   "source": [
    "## Download Steam Data\n",
    "\n",
    "`Now we are ready to start downloading data and writing to file. We define our logic particular to handling the steam API - in fact if no data is returned we return just the name and appid - then begin setting some parameters. We define the files we will write our data and index to, and the columns for the csv file. The API doesn't return every column for every app, so it is best to explicitly set these.`\n",
    "\n",
    "`Next we run our functions to set up the files, and make a call to process_batches to begin the process. Some additional parameters have been added for demonstration, to constrain the download to just a few rows and smaller batches. Removing these would allow the entire download process to be repeated.`\n",
    "\n",
    "I retouched many of these parameters just to check if the download could made in batches (requesting several steamapps at the same time), or even putting a faster polling rate (right now it is one second).\n",
    "\n",
    "The storefront API (http://store.steampowered.com/api/) is very much undocumented, but the key getaway is that is not possible. The official SteamWorks API lets us do other things and it is quite well documented, but we cannot get the data available at a steam webpage, which are the things interesting for us.\n",
    "\n",
    "The storefront API is only accessible with these [requests](https://wiki.teamfortress.com/wiki/User:RJackson/StorefrontAPI), and according to this [stackoverflow discussion](https://stackoverflow.com/questions/46330864/steam-api-all-games):\n",
    "`There is a general API rate limit for each unique IP adress of 200 requests in five minutes which is one request every 1.5 seconds.` This matches our experience, Nik Davis put a pause between requests of just 1 second, and with this we get some but a few reconnect errors. If we put no pause at all, at the end we are limited by the 200 requests every 5 minutes.\n",
    "\n",
    "That means that for a volume of around 50k at January 2022 (the steam apps available also at steam spy, already filtered by game and some owner data...) this download will take around 21 hours. Thankfully we can resume it and do it in several batches.\n",
    "\n",
    "If we were to build a web app and we wanted to update the information daily, we could instead try pulling the full applist from steam along with the \"last updated\" and only request the full appid information for those ids. This is probably what Steam Spy does, and SteamDB instead uses a more sofisticated approach by being notified of any changes to appids via steamworks.\n",
    "\n",
    "In any case, for a one shot analysis (and not a web page where the user could explore the information), the full download approach is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the next two functions is getting the full list of apps from Steam, and also getting from our already downloaded data (if any) which are the appids we already have, to get the \"delta\" so we only have to download the new app ids.\n",
    "\n",
    "We could retouch them a bit, as they have a \"last updated\" key, so we also update new information and not just new ids. But that would be more suitable for a live webpage updated once a day, not for a full on analysis like we are presenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAppListBatch(url, parameters):\n",
    "    json_data = get_request(url, parameters=parameters)\n",
    "    steam_id = pd.DataFrame.from_dict(json_data[\"response\"][\"apps\"])\n",
    "    try:\n",
    "        more_results = json_data[\"response\"][\"have_more_results\"]\n",
    "        last_appid =  json_data[\"response\"][\"last_appid\"]\n",
    "    except:\n",
    "        more_results = False\n",
    "        last_appid = False\n",
    "    return more_results, steam_id, last_appid\n",
    "\n",
    "def get_update_ids_old(updatedlist, oldlist):\n",
    "    updatedlist['key1'] = 1\n",
    "    oldlist['key2'] = 1\n",
    "    updatedlist = pd.merge(updatedlist, oldlist, right_on=['steam_appid','name'],left_on=['appid','name'], how = 'outer')\n",
    "    updatedlist = updatedlist[~(updatedlist.key2 == updatedlist.key1)]\n",
    "    updatedlist = updatedlist.drop(['key1','key2','steam_appid'], axis=1)\n",
    "    return updatedlist\n",
    "\n",
    "def get_update_ids(idList, oldFullList):\n",
    "    #We are going to forget about names and only care about IDs.\n",
    "    idList = idList[\"appid\"]\n",
    "    oldFullList = oldFullList[\"steam_appid\"]\n",
    "    oldFullList.columns = [\"appid\"]\n",
    "    updatedList = idList.append(oldFullList)\n",
    "    updatedList = updatedList.drop_duplicates(keep=False)\n",
    "    updatedList = updatedList.reset_index(drop=True)\n",
    "    return updatedList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAppList():\n",
    "    with open('../data/steam_key.txt') as f:\n",
    "        key = f.read()\n",
    "\n",
    "    url = \"https://api.steampowered.com/IStoreService/GetAppList/v1/?\"\n",
    "    parameters = {\"key\": key}\n",
    "    more_results = True\n",
    "    begin = True\n",
    "    # from the request we get the more_results flag and also the last_appid, so we use them for the next requests.\n",
    "    while (more_results):\n",
    "        more_results, steam_ids, last_appid = getAppListBatch(url, parameters)\n",
    "        parameters[\"last_appid\"] = last_appid\n",
    "        if (begin):\n",
    "            steam_allids = steam_ids\n",
    "            begin = False\n",
    "        else:\n",
    "            steam_allids = steam_allids.append(steam_ids)\n",
    "    return steam_allids\n",
    "# request 'all' from steam spy and parse into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "W7e3eUD2XTI8",
    "outputId": "3dced899-ac96-42b4-e779-2e7cf68c4234",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_steam_request(appid):\n",
    "    \"\"\"Unique parser to handle data from Steam Store API.\n",
    "    \n",
    "    Returns : json formatted data (dict-like)\n",
    "    \"\"\"\n",
    "    with open('../data/steam_key.txt') as f:\n",
    "        key = f.read()\n",
    "        \n",
    "    url = \"http://store.steampowered.com/api/appdetails/\"\n",
    "    parameters = {\"appids\": appid, \"key\": key}\n",
    "    \n",
    "    json_data = get_request(url, parameters=parameters)\n",
    "    json_app_data = json_data[str(appid)]\n",
    "    \n",
    "    if json_app_data['success']:\n",
    "        data = json_app_data['data']\n",
    "    else:\n",
    "        data = {'steam_appid': appid}\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# Set file parameters\n",
    "download_path = '../data/download/'\n",
    "steam_app_data = 'steam_app_data.csv'\n",
    "steam_app_data_delta = 'steam_app_data_delta.csv'\n",
    "steam_index = 'steam_index.txt'\n",
    "\n",
    "steam_columns = [\n",
    "    'type', 'name', 'steam_appid', 'required_age', 'is_free', 'controller_support',\n",
    "    'dlc', 'detailed_description', 'about_the_game', 'short_description', 'fullgame',\n",
    "    'supported_languages', 'header_image', 'website', 'pc_requirements', 'mac_requirements',\n",
    "    'linux_requirements', 'legal_notice', 'drm_notice', 'ext_user_account_notice',\n",
    "    'developers', 'publishers', 'demos', 'price_overview', 'packages', 'package_groups',\n",
    "    'platforms', 'metacritic', 'reviews', 'categories', 'genres', 'screenshots',\n",
    "    'movies', 'recommendations', 'achievements', 'release_date', 'support_info',\n",
    "    'background', 'content_descriptors'\n",
    "]\n",
    "\n",
    "# Overwrites last index for demonstration (would usually store highest index so can continue across sessions)\n",
    "if (os.path.isfile(download_path+steam_app_data_delta) == False):\n",
    "    reset_index(download_path, steam_index)\n",
    "\n",
    "# Retrieve last index downloaded from file\n",
    "index = get_index(download_path, steam_index)\n",
    "\n",
    "# Wipe or create data file and write headers if no previous  data\n",
    "if (os.path.isfile(download_path+steam_app_data) == False):\n",
    "    prepare_data_file(download_path, steam_app_data, index, steam_columns)\n",
    "    \n",
    "# Wipe or create data file delta and write headers if index is 0\n",
    "if (os.path.isfile(download_path+steam_app_data_delta) == False):\n",
    "    prepare_data_file(download_path, steam_app_data_delta, index, steam_columns)\n",
    "    \n",
    "    \n",
    "# Here we get the list of appids from steam\n",
    "full_steam_ids = getAppList()\n",
    "\n",
    "# Here we get the real list of ids not yet in our dataframe. If this is the first time we are downloading the data, we can skip\n",
    "# This step and instead use the full app_list.\n",
    "try:\n",
    "    oldlist = pd.read_csv('../data/download/steam_app_data.csv', usecols = ['name','steam_appid'])\n",
    "    steam_ids = get_update_ids(full_steam_ids, oldlist)\n",
    "except FileNotFoundError:\n",
    "    print(\"Pre-existing file not found. First time downloading full app data from steam. This will take a while.\\n\")\n",
    "    steam_ids = full_steam_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New IDs detected: 66368\n"
     ]
    }
   ],
   "source": [
    "print(\"New IDs detected: \"+str(len(steam_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_ids.to_csv('../data/download/steam_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "1-data-collection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
